# -*- coding: utf-8 -*-
"""streamlit_dashboard.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wYR2yMxj8YKfu7IqLSsKpdoRnMT-Cjd0
"""



# Commented out IPython magic to ensure Python compatibility.
# %pip install astroquery astropy lightkurve

import sys
print(sys.executable)

from astroquery.ipac.nexsci.nasa_exoplanet_archive import NasaExoplanetArchive
import pandas as pd

# -----------------------
# 1. Import libraries
# -----------------------
from astroquery.ipac.nexsci.nasa_exoplanet_archive import NasaExoplanetArchive
import pandas as pd

# -----------------------
# 2. Define columns we need
# -----------------------
cols = (
    "pl_name,hostname,disc_facility,discoverymethod,disc_year,"
    "st_teff,st_met,st_rotp,st_mass,st_rad,"
    "sy_pnum,sy_snum,sy_dist"
)

# -----------------------
# 3. Query Kepler planets
# -----------------------
kepler = NasaExoplanetArchive.query_criteria(
    table="pscomppars",
    select=cols,
    where="disc_facility like '%Kepler%'"
)
kepler_df = kepler.to_pandas()
kepler_df["survey"] = "Kepler"

# -----------------------
# 4. Query TESS planets
# -----------------------
tess = NasaExoplanetArchive.query_criteria(
    table="pscomppars",
    select=cols,
    where="disc_facility like '%TESS%'"
)
tess_df = tess.to_pandas()
tess_df["survey"] = "TESS"

# -----------------------
# 5. Preview results
# -----------------------
print("✅ Connected to NASA Exoplanet Archive")
print("Kepler planets:", len(kepler_df))
print("TESS planets:", len(tess_df))

display(kepler_df.head())
display(tess_df.head())

# -----------------------
# 1. Import libraries
# -----------------------
from astroquery.ipac.nexsci.nasa_exoplanet_archive import NasaExoplanetArchive
import pandas as pd

# -----------------------
# 2. Define columns we need
# -----------------------
cols = (
    "pl_name,hostname,disc_facility,discoverymethod,disc_year,"
    "st_teff,st_met,st_rotp,st_mass,st_rad,"
    "sy_pnum,sy_snum,sy_dist"
)

# -----------------------
# 3. Query Kepler planets
# -----------------------
kepler = NasaExoplanetArchive.query_criteria(
    table="pscomppars",
    select=cols,
    where="disc_facility like '%Kepler%'"
)
kepler_df = kepler.to_pandas()
kepler_df["survey"] = "Kepler"

# -----------------------
# 4. Query TESS planets
# -----------------------
tess = NasaExoplanetArchive.query_criteria(
    table="pscomppars",
    select=cols,
    where="disc_facility like '%TESS%'"
)
tess_df = tess.to_pandas()
tess_df["survey"] = "TESS"

# -----------------------
# 5. Save results to CSV
# -----------------------
kepler_df.to_csv("kepler_exoplanets.csv", index=False)
tess_df.to_csv("tess_exoplanets.csv", index=False)

# Optional: combine both into one file
all_exoplanets = pd.concat([kepler_df, tess_df], ignore_index=True)
all_exoplanets.to_csv("all_exoplanets.csv", index=False)

# -----------------------
# 6. Preview results
# -----------------------
print("✅ Connected to NASA Exoplanet Archive and saved data")
print("Kepler planets:", len(kepler_df))
print("TESS planets:", len(tess_df))
print("Total combined:", len(all_exoplanets))

display(all_exoplanets.head())

# Week 2: Cleaning and Merging Host-Star Parameters
import os
os.listdir()

import pandas as pd

# load planet catalogs
kepler_planets = pd.read_csv("kepler_exoplanets.csv", low_memory=False)
tess_planets = pd.read_csv("tess_exoplanets.csv", low_memory=False)

print("Kepler planets:", kepler_planets.shape)
print("TESS planets:", tess_planets.shape)

# peek columns
print(kepler_planets.columns[:20])
print(tess_planets.columns[:20])

# select key columns
kepler_clean = kepler_planets[['hostname', 'st_teff', 'st_met', 'st_rotp']].copy()
tess_clean = tess_planets[['hostname', 'st_teff', 'st_met', 'st_rotp']].copy()

# add mission column
kepler_clean['mission'] = 'Kepler'
tess_clean['mission'] = 'TESS'

merged_stars = pd.concat([kepler_clean, tess_clean], ignore_index=True)

print("Merged shape:", merged_stars.shape)
print(merged_stars.head())

merged_stars = merged_stars.drop_duplicates(subset=['hostname'])
print("Unique stars:", merged_stars.shape)

print(merged_stars['mission'].value_counts())

print("Missing metallicity:", merged_stars['st_met'].isna().sum())
print("Missing rotation period:", merged_stars['st_rotp'].isna().sum())

import matplotlib.pyplot as plt

# Histogram: Stellar Temperature
plt.hist(merged_stars['st_teff'].dropna(), bins=30, edgecolor='black')
plt.xlabel("Effective Temperature (K)")
plt.ylabel("Number of Stars")
plt.title("Distribution of Stellar Temperatures")
plt.show()

# Histogram: Stellar Metallicity
plt.hist(merged_stars['st_met'].dropna(), bins=30, edgecolor='black')
plt.xlabel("Metallicity [Fe/H]")
plt.ylabel("Number of Stars")
plt.title("Distribution of Stellar Metallicities")
plt.show()

# Pie chart

# Pie chart: Mission Contribution
mission_counts = merged_stars['mission'].value_counts()
plt.pie(mission_counts, labels=mission_counts.index, autopct='%1.1f%%', startangle=90)
plt.title("Kepler vs TESS Host Stars")
plt.show()

# Week 3: Added automated cleaning pipeline and cleaned dataset
import pandas as pd

def clean_exoplanet_data(kepler_file, tess_file, output_file, convert_units=False):
    """
    Automated data-cleaning pipeline for Kepler and TESS datasets.

    Steps:
    1. Load datasets.
    2. Add 'mission' column.
    3. Merge and deduplicate by host star.
    4. Handle missing metallicity and distance.
    5. Drop sparse rotation column.
    6. Optional unit conversions.
    7. Save cleaned CSV for reproducibility.
    """
    # Load CSVs
    kepler = pd.read_csv(kepler_file)
    tess   = pd.read_csv(tess_file)

    # Add source column
    kepler['mission'] = 'Kepler'
    tess['mission']   = 'TESS'

    # Merge datasets
    merged = pd.concat([kepler, tess], ignore_index=True)
    merged = merged.drop_duplicates(subset='hostname')

    # Drop sparse rotation column
    if 'st_rotp' in merged.columns:
        merged = merged.drop(columns=['st_rotp'])

    # Fill missing values
    if 'st_met' in merged.columns:
        merged['st_met'] = merged['st_met'].fillna(merged['st_met'].median())
    if 'sy_dist' in merged.columns:
        merged['sy_dist'] = merged['sy_dist'].fillna(merged['sy_dist'].median())

    # Optional unit conversions
    if convert_units:
        merged['sy_dist_ly'] = merged['sy_dist'] * 3.26156
        merged['st_mass_kg'] = merged['st_mass'] * 1.9885e30
        merged['st_rad_m']   = merged['st_rad'] * 6.957e8

    # Keep relevant columns
    cols = ['pl_name', 'hostname', 'st_teff', 'st_met', 'st_mass', 'st_rad',
            'sy_dist', 'disc_year', 'discoverymethod', 'mission']
    if convert_units:
        cols += ['sy_dist_ly', 'st_mass_kg', 'st_rad_m']

    clean_df = merged[cols]

    # Save cleaned CSV
    clean_df.to_csv(output_file, index=False)
    print(f"Cleaning complete! Saved to {output_file}")

    return clean_df

# --- Example usage ---
kepler_file = 'kepler_exoplanets.csv'
tess_file   = 'tess_exoplanets.csv'
output_file = 'clean_exoplanets.csv'

clean_data = clean_exoplanet_data(kepler_file, tess_file, output_file, convert_units=True)

# WEEK 4: Clean datasets and compute planet occurrence per host star
import pandas as pd
from datetime import datetime

def clean_and_count_planets(kepler_file, tess_file, output_file, convert_units=True):
    """
    Clean Kepler and TESS datasets and compute true planet occurrence per host star.
    """
    # --- 1. Load CSVs ---
    kepler = pd.read_csv(kepler_file)
    tess   = pd.read_csv(tess_file)

    # Add mission/source column
    kepler['mission'] = 'Kepler'
    tess['mission']   = 'TESS'

    # --- 2. Merge datasets ---
    merged = pd.concat([kepler, tess], ignore_index=True)

    # --- 3. Drop sparse rotation column if exists ---
    if 'st_rotp' in merged.columns:
        merged = merged.drop(columns=['st_rotp'])

    # --- 4. Fill missing values ---
    if 'st_met' in merged.columns:
        merged['st_met'] = merged['st_met'].fillna(merged['st_met'].median())
    if 'sy_dist' in merged.columns:
        merged['sy_dist'] = merged['sy_dist'].fillna(merged['sy_dist'].median())

    # --- 5. Optional unit conversions ---
    if convert_units:
        merged['sy_dist_ly'] = merged['sy_dist'] * 3.26156
        merged['st_mass_kg'] = merged['st_mass'] * 1.9885e30
        merged['st_rad_m']   = merged['st_rad'] * 6.957e8

    # --- 6. Count planets per host star ---
    planet_counts = merged.groupby('hostname').size().reset_index(name='planet_count')

    # --- 7. Keep one row per star and merge counts ---
    cols = ['hostname', 'st_teff', 'st_met', 'st_mass', 'st_rad',
            'sy_dist', 'disc_year', 'discoverymethod', 'mission']
    if convert_units:
        cols += ['sy_dist_ly', 'st_mass_kg', 'st_rad_m']

    # Keep only one row per host star for modeling
    merged_unique = merged.drop_duplicates(subset='hostname')[cols].copy()

    # Merge planet counts
    merged_unique = pd.merge(merged_unique, planet_counts, on='hostname', how='left')

    # --- 8. Add derived columns ---
    # Metallicity class
    bins = [-float('inf'), -0.5, 0.0, 0.5, float('inf')]
    labels = ['low', 'sub-solar', 'solar', 'high']
    merged_unique['metallicity_class'] = pd.cut(merged_unique['st_met'], bins=bins, labels=labels)

    # Spectral type by temperature
    temp_bins = [0, 4000, 5200, 6000, 7500, float('inf')]
    temp_labels = ['M', 'K', 'G', 'F', 'A']
    merged_unique['spectral_type'] = pd.cut(merged_unique['st_teff'], bins=temp_bins, labels=temp_labels)

    # --- 9. Add timestamp ---
    merged_unique['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    # --- 10. Save final dataset ---
    merged_unique.to_csv(output_file, index=False)
    print(f"Final cleaned dataset with true planet counts saved to {output_file}")

    return merged_unique

# --- Run the updated pipeline ---
kepler_file = 'kepler_exoplanets.csv'
tess_file   = 'tess_exoplanets.csv'
output_file = 'final_occurrence_dataset_multi.csv'

final_dataset = clean_and_count_planets(kepler_file, tess_file, output_file, convert_units=True)

import pandas as pd

# Load the updated dataset
df = pd.read_csv('final_occurrence_dataset_multi.csv')

# Count stars by number of planets
planet_count_summary = df['planet_count'].value_counts().sort_index()

print("Planet Count Distribution:")
print(planet_count_summary)

# Percentage of multi-planet systems
total_stars = len(df)
multi_planet_stars = planet_count_summary[planet_count_summary.index > 1].sum()
percent_multi = (multi_planet_stars / total_stars) * 100

print(f"\nTotal stars: {total_stars}")
print(f"Stars with >1 planet: {multi_planet_stars} ({percent_multi:.2f}%)")

import pandas as pd

df = pd.read_csv('final_occurrence_dataset_multi.csv')

# --- 1. Normalized Stellar Properties (optional but helps modeling) ---
df['st_teff_norm'] = (df['st_teff'] - df['st_teff'].mean()) / df['st_teff'].std()
df['st_met_norm']  = (df['st_met'] - df['st_met'].mean()) / df['st_met'].std()
df['st_mass_norm'] = (df['st_mass'] - df['st_mass'].mean()) / df['st_mass'].std()
df['st_rad_norm']  = (df['st_rad'] - df['st_rad'].mean()) / df['st_rad'].std()

# --- 2. Detection Survey Flags (if needed for Bayesian priors) ---
# Already have 'mission' column: Kepler/TESS

# --- 3. Optional: Planet occurrence categories ---
# e.g., low, medium, high number of planets
bins = [0, 1, 3, float('inf')]
labels = ['single', 'few', 'many']
df['planet_occurrence_class'] = pd.cut(df['planet_count'], bins=bins, labels=labels)

# --- 4. Save the final pre-Bayesian dataset ---
df.to_csv('final_dataset_pre_bayesian.csv', index=False)
print("Final dataset pre-Bayesian saved as 'final_dataset_pre_bayesian.csv'")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('final_dataset_pre_bayesian.csv')

# --- 1. Planet Count Histogram ---
plt.figure(figsize=(8,5))
sns.countplot(x='planet_count', data=df)  # Removed palette
plt.title("Distribution of Planet Counts per Star")
plt.xlabel("Number of Planets")
plt.ylabel("Number of Stars")
plt.show()

# --- 2. Histogram of Stellar Metallicity ---
plt.figure(figsize=(8,5))
sns.histplot(df['st_met'], bins=30, kde=True, color='teal')
plt.title("Distribution of Stellar Metallicity")
plt.xlabel("Stellar Metallicity [dex]")
plt.ylabel("Number of Stars")
plt.show()

# --- 3. Boxplot: Planet Count vs Metallicity ---
plt.figure(figsize=(8,5))
sns.boxplot(x='planet_count', y='st_met', data=df)  # Removed palette
plt.title("Planet Count vs Stellar Metallicity")
plt.xlabel("Planet Count")
plt.ylabel("Metallicity [dex]")
plt.show()

# --- 4. Boxplot: Planet Count vs Stellar Temperature ---
plt.figure(figsize=(8,5))
sns.boxplot(x='planet_count', y='st_teff', data=df)  # Removed palette
plt.title("Planet Count vs Stellar Temperature")
plt.xlabel("Planet Count")
plt.ylabel("Temperature [K]")
plt.show()

# --- 5. Heatmap: Mean Planet Count by Spectral Type & Metallicity Class ---
pivot = df.pivot_table(values='planet_count', index='spectral_type', columns='metallicity_class', aggfunc='mean')
plt.figure(figsize=(8,6))
sns.heatmap(pivot, annot=True, fmt=".2f", cmap='viridis')
plt.title("Mean Planet Count by Spectral Type and Metallicity Class")
plt.show()

import pandas as pd
#QA PART#

df = pd.read_csv('final_dataset_pre_bayesian.csv')

print("=== Dataset Info ===")
print(df.info())

print("\n=== Check for Missing Values ===")
print(df.isnull().sum())

print("\n=== Basic Descriptive Statistics ===")
print(df[['planet_count', 'st_met', 'st_teff', 'st_mass', 'st_rad']].describe())

print("\n=== Planet Count Distribution ===")
print(df['planet_count'].value_counts().sort_index())

print("\n=== Multi-Planet Systems ===")
total_stars = len(df)
multi_planet_stars = df[df['planet_count'] > 1].shape[0]
percent_multi = (multi_planet_stars / total_stars) * 100
print(f"Total stars: {total_stars}")
print(f"Stars with >1 planet: {multi_planet_stars} ({percent_multi:.2f}%)")

print("\n=== Check Derived Columns ===")
print("Metallicity classes:", df['metallicity_class'].unique())
print("Spectral types:", df['spectral_type'].unique())
print("Planet occurrence classes:", df['planet_occurrence_class'].unique())

print("\n=== Check Normalized Columns ===")
print(df[['st_teff_norm', 'st_met_norm', 'st_mass_norm', 'st_rad_norm']].head())

import pandas as pd

# QA PART

df = pd.read_csv('final_dataset_pre_bayesian.csv')

print("=== Dataset Info ===")
print(df.info())

print("\n=== Check for Missing Values ===")
print(df.isnull().sum())

print("\n=== Basic Descriptive Statistics ===")
print(df[['planet_count', 'st_met', 'st_teff', 'st_mass', 'st_rad']].describe())

print("\n=== Planet Count Distribution ===")
print(df['planet_count'].value_counts().sort_index())

print("\n=== Multi-Planet Systems ===")
total_stars = len(df)
multi_planet_stars = df[df['planet_count'] > 1].shape[0]
percent_multi = (multi_planet_stars / total_stars) * 100
print(f"Total stars: {total_stars}")
print(f"Stars with >1 planet: {multi_planet_stars} ({percent_multi:.2f}%)")

print("\n=== Check Derived Columns ===")
print("Metallicity classes:", df['metallicity_class'].unique())
print("Spectral types:", df['spectral_type'].unique())
print("Planet occurrence classes:", df['planet_occurrence_class'].unique())

print("\n=== Check Normalized Columns ===")
print(df[['st_teff_norm', 'st_met_norm', 'st_mass_norm', 'st_rad_norm']].head())



import pymc as pm
import arviz as az
import numpy as np
import pandas as pd

# Load pre-Bayesian dataset
df = pd.read_csv('final_dataset_pre_bayesian.csv')

# Use normalized features
X_teff = df['st_teff_norm'].values
X_met  = df['st_met_norm'].values
X_mass = df['st_mass_norm'].values
planet_counts = df['planet_count'].values

with pm.Model() as model:
    # Priors for coefficients
    beta_teff = pm.Normal("beta_teff", mu=0, sigma=1)
    beta_met  = pm.Normal("beta_met", mu=0, sigma=1)
    beta_mass = pm.Normal("beta_mass", mu=0, sigma=1)

    # Intercept
    alpha = pm.Normal("alpha", mu=0, sigma=1)

    # Linear model
    lambda_ = pm.math.exp(alpha + beta_teff*X_teff + beta_met*X_met + beta_mass*X_mass)

    # Likelihood (Poisson for counts)
    y_obs = pm.Poisson("y_obs", mu=lambda_, observed=planet_counts)

    # Sample from posterior
    trace = pm.sample(2000, tune=1000, target_accept=0.9, return_inferencedata=True)

# Diagnostics & posterior plots
az.plot_trace(trace, var_names=["alpha", "beta_teff", "beta_met", "beta_mass"])
az.summary(trace, var_names=["alpha", "beta_teff", "beta_met", "beta_mass"])

import pandas as pd

df = pd.read_csv('final_dataset_pre_bayesian.csv')
print(df.columns)

import pymc as pm
import arviz as az
import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv('final_dataset_pre_bayesian.csv')

# Encode survey as index for hierarchical model
survey_codes, survey_idx = np.unique(df['mission'], return_inverse=True)

# Predictors
X_met  = df['st_met_norm'].values
X_teff = df['st_teff_norm'].values
X_mass = df['st_mass_norm'].values
X_rad  = df['st_rad_norm'].values
y_planets = df['planet_count'].values

# Number of surveys
n_survey = len(survey_codes)

with pm.Model() as hierarchical_model:

    # Hyperpriors for survey-level intercepts
    mu_a = pm.Normal("mu_a", mu=0, sigma=1)
    sigma_a = pm.Exponential("sigma_a", 1.0)

    # Survey-level intercepts
    a_survey = pm.Normal("a_survey", mu=mu_a, sigma=sigma_a, shape=n_survey)

    # Coefficients for predictors (fixed effects)
    beta_met  = pm.Normal("beta_met", mu=0, sigma=1)
    beta_teff = pm.Normal("beta_teff", mu=0, sigma=1)
    beta_mass = pm.Normal("beta_mass", mu=0, sigma=1)
    beta_rad  = pm.Normal("beta_rad", mu=0, sigma=1)

    # Linear model with survey-level intercept
    lambda_ = pm.math.exp(a_survey[survey_idx] +
                          beta_met*X_met +
                          beta_teff*X_teff +
                          beta_mass*X_mass +
                          beta_rad*X_rad)

    # Likelihood: Poisson
    y_obs = pm.Poisson("y_obs", mu=lambda_, observed=y_planets)

    # Sample posterior
    trace = pm.sample(2000, tune=1000, target_accept=0.9, return_inferencedata=True)

# Diagnostics
az.plot_trace(trace, var_names=["mu_a","sigma_a","beta_met","beta_teff","beta_mass","beta_rad"])
az.summary(trace, var_names=["mu_a","sigma_a","beta_met","beta_teff","beta_mass","beta_rad"])

import arviz as az
import plotly.express as px
import pandas as pd

# --- Posterior object from Step 1 ---
posterior = trace.posterior

# =========================
# 1️⃣ Trace Plots for Convergence
# =========================
coef_cols = ["beta_met","beta_teff","beta_mass","beta_rad"]
az.plot_trace(trace, var_names=coef_cols + ["mu_a","sigma_a"])
az.plot_trace(trace, var_names=["a_survey"])
# Optional: plt.show() if needed

# =========================
# 2️⃣ Posterior Summary with 95% HDI
# =========================
summary_df = az.summary(trace, var_names=coef_cols + ["mu_a","sigma_a"], hdi_prob=0.95)
print(summary_df)
summary_df.to_csv("posterior_coefficients_summary.csv", index=True)

# =========================
# 3️⃣ Flatten posterior for interactive plotting
# =========================
posterior_flat = posterior.to_dataframe().reset_index()

# =========================
# 4️⃣ Interactive Posterior Histograms for Fixed-Effect Coefficients
# =========================
for coef in coef_cols:
    samples = posterior_flat[coef].values
    hdi = az.hdi(samples, hdi_prob=0.95)

    fig = px.histogram(samples, nbins=50, marginal="box",
                       title=f"Posterior Distribution of {coef}",
                       labels={coef: coef})
    # Add 95% HDI lines
    fig.add_vline(x=hdi[0], line_dash="dash", line_color="red", annotation_text="HDI 2.5%")
    fig.add_vline(x=hdi[1], line_dash="dash", line_color="red", annotation_text="HDI 97.5%")
    fig.show()

# =========================
# 5️⃣ Survey-Level Intercepts (Combined Violin Plot) - Fixed
# =========================
df_data = pd.read_csv('final_dataset_pre_bayesian.csv')
survey_names = df_data['mission'].unique()

survey_samples = []
for i, survey in enumerate(survey_names):
    vals = posterior_flat.loc[posterior_flat['a_survey_dim_0'] == i, 'a_survey'].values
    survey_samples.append(pd.DataFrame({"survey": survey, "value": vals}))

survey_df = pd.concat(survey_samples, ignore_index=True)

fig = px.violin(survey_df, x="survey", y="value", box=True, points="all",
                title="Posterior Distribution of Survey-Level Intercepts",
                labels={"value": "Intercept"})
fig.show()

# ===============================
# Step 3: Posterior Predictive Checks
# ===============================

import pymc as pm
import arviz as az
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px

# --- 1️⃣ Posterior predictive sampling ---
with hierarchical_model:
    ppc = pm.sample_posterior_predictive(trace, var_names=["y_obs"], random_seed=42)

# --- 2️⃣ Prepare posterior predictive samples ---
# Stack chain and draw dimensions -> shape (n_samples, n_stars)
ppc_samples = ppc.posterior_predictive["y_obs"].stack(sample=("chain","draw")).values.T

# Compute mean and 95% HDI per star
ppc_mean = ppc_samples.mean(axis=0)          # mean predicted planet count per star
ppc_hdi = az.hdi(ppc_samples, hdi_prob=0.95) # 95% credible interval per star

# --- 3️⃣ Observed planet counts ---
df_data = pd.read_csv('final_dataset_pre_bayesian.csv')
observed = df_data['planet_count'].values

# Safety check
assert len(observed) == len(ppc_mean) == len(ppc_hdi), "Array lengths do not match!"

# =========================
# 4️⃣ Matplotlib Histogram Overlay: Observed vs Predicted
# =========================
plt.figure(figsize=(10,6))
plt.hist(observed, bins=np.arange(observed.max()+2)-0.5, alpha=0.5, label="Observed", color="blue")
plt.hist(ppc_mean, bins=np.arange(observed.max()+2)-0.5, alpha=0.5, label="Predicted (posterior mean)", color="orange")
plt.xlabel("Planet count per star")
plt.ylabel("Number of stars")
plt.title("Posterior Predictive Check: Observed vs Predicted Planet Counts")
plt.legend()
plt.show()

# =========================
# 5️⃣ Interactive Plotly Scatter: Observed vs Predicted with 95% CI
# =========================
ppc_df = pd.DataFrame({
    "Observed": observed,
    "Predicted_mean": ppc_mean,
    "Predicted_HDI_low": ppc_hdi[:,0],
    "Predicted_HDI_high": ppc_hdi[:,1]
})

fig = px.scatter(ppc_df, x="Observed", y="Predicted_mean",
                 error_y=ppc_df["Predicted_HDI_high"]-ppc_df["Predicted_mean"],
                 error_y_minus=ppc_df["Predicted_mean"]-ppc_df["Predicted_HDI_low"],
                 labels={"Observed":"Observed planet count",
                         "Predicted_mean":"Predicted mean planet count"},
                 title="Posterior Predictive Check: Observed vs Predicted with 95% CI")

# Add 1:1 reference line
fig.add_shape(
    type="line", line=dict(dash="dash", color="red"),
    x0=0, x1=ppc_df["Observed"].max(),
    y0=0, y1=ppc_df["Observed"].max()
)
fig.show()

# ===============================
# Step 4: Model Comparison (Full Dashboard)
# ===============================
import pymc as pm
import arviz as az
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# --- Load / prepare data ---
df = pd.read_csv("final_dataset_pre_bayesian.csv")
X_met = df["st_met_norm"].values
X_teff = df["st_teff_norm"].values
X_mass = df["st_mass_norm"].values
X_rad = df["st_rad_norm"].values
y_planets = df["planet_count"].values

survey_codes, survey_idx = np.unique(df["mission"], return_inverse=True)
n_survey = len(survey_codes)

# ===============================
# 1️⃣ Single Predictor Model
# ===============================
with pm.Model() as single_model:
    alpha = pm.Normal("alpha", mu=0, sigma=0.5)
    beta_met = pm.Normal("beta_met", mu=0, sigma=0.5)
    lambda_ = pm.math.exp(alpha + beta_met * X_met)
    y_obs = pm.Poisson("y_obs", mu=lambda_, observed=y_planets)
    idata_single = pm.sample(draws=1000, tune=1000, target_accept=0.99,
                             return_inferencedata=True, idata_kwargs={"log_likelihood": True},
                             random_seed=42)
    idata_single_ppc = pm.sample_posterior_predictive(idata_single, random_seed=42, return_inferencedata=True)

# -----------------------------
# Trace and PPC plots
# -----------------------------
az.plot_trace(idata_single, var_names=["alpha", "beta_met"], compact=True)
plt.show()
az.plot_ppc(idata_single_ppc)
plt.show()

# ===============================
# 2️⃣ Multi-Predictor Model
# ===============================
with pm.Model() as multi_model:
    alpha = pm.Normal("alpha", mu=0, sigma=0.5)
    beta_met = pm.Normal("beta_met", mu=0, sigma=0.5)
    beta_teff = pm.Normal("beta_teff", mu=0, sigma=0.5)
    beta_mass = pm.Normal("beta_mass", mu=0, sigma=0.5)
    beta_rad = pm.Normal("beta_rad", mu=0, sigma=0.5)
    lambda_ = pm.math.exp(alpha + beta_met*X_met + beta_teff*X_teff + beta_mass*X_mass + beta_rad*X_rad)
    y_obs = pm.Poisson("y_obs", mu=lambda_, observed=y_planets)
    idata_multi = pm.sample(draws=1000, tune=1000, target_accept=0.99,
                            return_inferencedata=True, idata_kwargs={"log_likelihood": True},
                            random_seed=42)
    idata_multi_ppc = pm.sample_posterior_predictive(idata_multi, random_seed=42, return_inferencedata=True)

# Trace and PPC plots
az.plot_trace(idata_multi, var_names=["alpha","beta_met","beta_teff","beta_mass","beta_rad"], compact=True)
plt.show()
az.plot_ppc(idata_multi_ppc)
plt.show()

# ===============================
# 3️⃣ Hierarchical Model
# ===============================
with pm.Model() as hier_model:
    mu_alpha = pm.Normal("mu_alpha", mu=0, sigma=1)
    sigma_alpha = pm.HalfNormal("sigma_alpha", sigma=1)
    alpha_offset = pm.Normal("alpha_offset", mu=0, sigma=1, shape=n_survey)
    alpha_group = pm.Deterministic("alpha_group", mu_alpha + alpha_offset * sigma_alpha)
    beta_met = pm.Normal("beta_met", mu=0, sigma=0.5)
    beta_teff = pm.Normal("beta_teff", mu=0, sigma=0.5)
    beta_mass = pm.Normal("beta_mass", mu=0, sigma=0.5)
    beta_rad = pm.Normal("beta_rad", mu=0, sigma=0.5)
    lambda_ = pm.math.exp(alpha_group[survey_idx] + beta_met*X_met + beta_teff*X_teff + beta_mass*X_mass + beta_rad*X_rad)
    y_obs = pm.Poisson("y_obs", mu=lambda_, observed=y_planets)
    idata_hier = pm.sample(draws=1000, tune=1000, target_accept=0.99,
                           return_inferencedata=True, idata_kwargs={"log_likelihood": True},
                           random_seed=42)
    idata_hier_ppc = pm.sample_posterior_predictive(idata_hier, random_seed=42, return_inferencedata=True)

# Trace and PPC plots
az.plot_trace(idata_hier, var_names=["mu_alpha","sigma_alpha","beta_met","beta_teff","beta_mass","beta_rad"], compact=True)
plt.show()
az.plot_ppc(idata_hier_ppc)
plt.show()

# ===============================
# 4️⃣ WAIC / LOO and Model Comparison
# ===============================
waic_single = az.waic(idata_single)
waic_multi  = az.waic(idata_multi)
waic_hier   = az.waic(idata_hier)

loo_single = az.loo(idata_single)
loo_multi  = az.loo(idata_multi)
loo_hier   = az.loo(idata_hier)

print("WAIC Single:", waic_single)
print("WAIC Multi:", waic_multi)
print("WAIC Hier:", waic_hier)

print("LOO Single:", loo_single)
print("LOO Multi:", loo_multi)
print("LOO Hier:", loo_hier)

# Compare models (WAIC)
compare_df = az.compare(
    {"Single": idata_single, "Multi": idata_multi, "Hierarchical": idata_hier},
    ic="waic", method="BB-pseudo-BMA", scale="log"
)
print(compare_df)
compare_df.to_csv("model_comparison_WAIC.csv", index=True)

az.plot_compare(compare_df, insample_dev=False)
plt.title("WAIC Model Comparison")
plt.show()

# ===============================
# Step 5: Uncertainty & Bias (Interactive, first plot auto)
# ===============================

import pymc as pm
import numpy as np
import pandas as pd
import plotly.graph_objects as go
from ipywidgets import interact, FloatSlider, VBox

# --- Load dataset ---
df = pd.read_csv("final_dataset_pre_bayesian.csv")

# Create detection flag
df["detected"] = (df["planet_count"] > 0).astype(int)

# Predictor variables
X_met = df["st_met_norm"].values
X_teff = df["st_teff_norm"].values
y = df["detected"].values
mission = df["mission"].values

# -----------------------------
# Function to run model and plot posteriors safely
# -----------------------------
def plot_bias_posteriors(det_eff_kepler=0.8, det_eff_tess=0.6):
    with pm.Model() as model:
        # Priors
        alpha = pm.Normal("alpha", mu=0, sigma=1)
        beta_met = pm.Normal("beta_met", mu=0, sigma=1)
        beta_teff = pm.Normal("beta_teff", mu=0, sigma=1)

        # Linear predictor
        logit_p = alpha + beta_met * X_met + beta_teff * X_teff
        p = pm.math.sigmoid(logit_p)

        # Detection efficiency per mission
        det_eff = np.where(mission == "Kepler", det_eff_kepler, det_eff_tess)
        p_detect = p * det_eff

        # Likelihood
        pm.Bernoulli("y_obs", p=p_detect, observed=y)

        # Sample posterior
        trace = pm.sample(300, tune=300, chains=2, target_accept=0.9, progressbar=False, random_seed=42)

    # Convert posterior to DataFrame safely
    posterior_df = trace.posterior.to_dataframe().reset_index()
    posterior_df = posterior_df.loc[:, ~posterior_df.columns.duplicated()]  # drop duplicates

    # Plot histogram for all coefficients
    fig = go.Figure()
    for coef in ["alpha", "beta_met", "beta_teff"]:
        fig.add_trace(go.Histogram(
            x=posterior_df[coef],
            name=coef,
            opacity=0.6,
            nbinsx=40
        ))

    fig.update_layout(
        barmode="overlay",
        title=f"Posterior Distributions (Kepler Eff={det_eff_kepler}, TESS Eff={det_eff_tess})",
        xaxis_title="Coefficient Value",
        yaxis_title="Frequency",
        legend_title="Coefficients"
    )
    fig.show()

# -----------------------------
# Show first plot automatically
# -----------------------------
plot_bias_posteriors(det_eff_kepler=0.8, det_eff_tess=0.6)

# -----------------------------
# Interactive sliders
# -----------------------------
interact(
    plot_bias_posteriors,
    det_eff_kepler=FloatSlider(value=0.8, min=0.1, max=1.0, step=0.05, description="Kepler Eff"),
    det_eff_tess=FloatSlider(value=0.6, min=0.1, max=1.0, step=0.05, description="TESS Eff")
)

import numpy as np
import matplotlib.pyplot as plt
from ipywidgets import interact, FloatSlider, HTML
import arviz as az

# Posterior samples from your hierarchical model
beta_met_samples  = trace.posterior["beta_met"].values.flatten()
beta_teff_samples = trace.posterior["beta_teff"].values.flatten()
alpha_samples     = trace.posterior["mu_a"].values.flatten()   # use global mean intercept

# Function to compute predicted relative occurrence
def predict_occurrence(delta_fe, delta_teff):
    log_rel = alpha_samples + beta_met_samples * delta_fe + beta_teff_samples * delta_teff
    rel_occ = np.exp(log_rel)   # convert log-odds scale → multiplicative factor

    rel_mean = rel_occ.mean()
    hdi_low, hdi_high = az.hdi(rel_occ, hdi_prob=0.95)  # 95% credible interval
    return rel_mean, hdi_low, hdi_high, rel_occ

# Interactive interpretation
def update_interpretation(delta_fe, delta_teff):
    rel_mean, hdi_low, hdi_high, rel_occ = predict_occurrence(delta_fe, delta_teff)

    # Text output
    text = f"With metallicity Δ[Fe/H]={delta_fe:.2f} and ΔTeff={delta_teff:.2f} (norm), " \
           f"predicted planet occurrence is ~{rel_mean:.2f}× " \
           f"(95% CI: {hdi_low:.2f} – {hdi_high:.2f})."
    display(HTML(f"<b>{text}</b>"))

    # Mini histogram plot
    plt.figure(figsize=(6,3))
    plt.hist(rel_occ, bins=30, color="skyblue", alpha=0.7)
    plt.axvline(rel_mean, color='red', linestyle='--', label='Mean')
    plt.xlabel("Relative Planet Occurrence")
    plt.ylabel("Posterior samples")
    plt.title("Predicted Occurrence Distribution")
    plt.legend()
    plt.show()

# Sliders
interact(
    update_interpretation,
    delta_fe=FloatSlider(value=0.3, min=-0.5, max=0.8, step=0.05, description="[Fe/H] Δ"),
    delta_teff=FloatSlider(value=-0.3, min=-1.0, max=1.0, step=0.05, description="ΔTeff (norm)")
)

import numpy as np
import plotly.graph_objects as go

# Posterior samples
alpha_samples = trace.posterior['mu_a'].values.flatten()   # global intercept instead of alpha
beta_met_samples = trace.posterior['beta_met'].values.flatten()
beta_teff_samples = trace.posterior['beta_teff'].values.flatten()

# Create grid of metallicity and temperature
fe_grid = np.linspace(-0.5, 0.8, 30)
teff_grid = np.linspace(-1, 1, 30)
FE, TEFF = np.meshgrid(fe_grid, teff_grid)

# Compute mean predicted planet occurrence (posterior mean)
lambda_grid = np.zeros(FE.shape)
for i in range(FE.shape[0]):
    for j in range(FE.shape[1]):
        log_lambda = alpha_samples + beta_met_samples * FE[i, j] + beta_teff_samples * TEFF[i, j]
        lambda_grid[i, j] = np.exp(log_lambda.mean())

# 3D Surface Plot
fig = go.Figure(data=[go.Surface(z=lambda_grid, x=FE, y=TEFF)])
fig.update_layout(
    title='Predicted Planet Occurrence (Posterior Mean)',
    scene=dict(
        xaxis_title='Metallicity [Fe/H]',
        yaxis_title='Normalized Teff',
        zaxis_title='Predicted Planet Count'
    )
)
fig.show()

def scenario_prediction(fe, teff):
    log_lambda = alpha_samples + beta_met_samples*fe + beta_teff_samples*teff
    prob = 1 - np.exp(-np.exp(log_lambda))  # probability of ≥1 planet assuming Poisson
    mean_prob = prob.mean()
    hdi_low = np.percentile(prob, 2.5)
    hdi_high = np.percentile(prob, 97.5)
    print(f"For [Fe/H]={fe}, Teff={teff} (norm):")
    print(f"Predicted probability of hosting ≥1 planet: {mean_prob:.2f} (95% CI: {hdi_low:.2f}-{hdi_high:.2f})")

# Example:
scenario_prediction(fe=0.2, teff=0.0)

import ipywidgets as widgets
from IPython.display import display

def explore_priors(alpha_sigma=0.5, beta_sigma=0.5):
    print(f"Running model with alpha_sigma={alpha_sigma}, beta_sigma={beta_sigma}")
    # Here you could re-run a simplified model or just illustrate prior shapes
    x = np.linspace(-3,3,100)
    import matplotlib.pyplot as plt
    plt.figure(figsize=(6,3))
    plt.plot(x, 1/(np.sqrt(2*np.pi*alpha_sigma**2)) * np.exp(-0.5*(x/alpha_sigma)**2), label=f'alpha prior σ={alpha_sigma}')
    plt.plot(x, 1/(np.sqrt(2*np.pi*beta_sigma**2)) * np.exp(-0.5*(x/beta_sigma)**2), label=f'beta prior σ={beta_sigma}')
    plt.title("Dynamic Priors Visualization")
    plt.legend()
    plt.show()

widgets.interact(
    explore_priors,
    alpha_sigma=widgets.FloatSlider(min=0.1,max=2,value=0.5,step=0.1,description="Alpha σ"),
    beta_sigma=widgets.FloatSlider(min=0.1,max=2,value=0.5,step=0.1,description="Beta σ")
)

# ===============================
# Step 7: Cool Extras (Hierarchical Version, Fixed)
# ===============================

import numpy as np
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import ipywidgets as widgets
from IPython.display import display

# --- Posterior samples from your hierarchical model ---
alpha_samples = trace.posterior['mu_a'].values.flatten()  # population-level intercept
beta_met_samples = trace.posterior['beta_met'].values.flatten()
beta_teff_samples = trace.posterior['beta_teff'].values.flatten()

# --- Stellar Teff scaling: convert real to normalized if needed ---
teff_mean = df['st_teff'].mean()
teff_std = df['st_teff'].std()

def real_to_norm_teff(teff_real):
    return (teff_real - teff_mean) / teff_std

# --- 1️⃣ 3D Posterior Explorer ---
def plot_3d_explorer():
    fe_grid = np.linspace(-0.5, 0.8, 30)
    teff_grid = np.linspace(-1, 1, 30)
    FE, TEFF = np.meshgrid(fe_grid, teff_grid)
    lambda_grid = np.zeros(FE.shape)

    for i in range(FE.shape[0]):
        for j in range(FE.shape[1]):
            log_lambda = alpha_samples + beta_met_samples*FE[i,j] + beta_teff_samples*TEFF[i,j]
            lambda_grid[i,j] = np.exp(log_lambda.mean())

    fig = go.Figure(data=[go.Surface(z=lambda_grid, x=FE, y=TEFF)])
    fig.update_layout(
        title='Predicted Planet Occurrence (Posterior Mean)',
        scene=dict(
            xaxis_title='Metallicity [Fe/H]',
            yaxis_title='Normalized Teff',
            zaxis_title='Predicted Planet Count'
        )
    )
    fig.show()

# --- 2️⃣ Scenario Simulation with real Teff option ---
def scenario_prediction(fe=0.2, teff=6000, use_real=True):
    teff_norm = real_to_norm_teff(teff) if use_real else teff
    log_lambda = alpha_samples + beta_met_samples*fe + beta_teff_samples*teff_norm
    prob = 1 - np.exp(-np.exp(log_lambda))  # probability of ≥1 planet (Poisson)
    mean_prob = prob.mean()
    hdi_low = np.percentile(prob, 2.5)
    hdi_high = np.percentile(prob, 97.5)

    teff_display = f"{teff:.0f} K" if use_real else f"{teff_norm:.2f}"

    print(f"For [Fe/H]={fe}, Teff={teff_display} (norm={teff_norm:.2f}):")
    print(f"Predicted probability of hosting ≥1 planet: {mean_prob:.2f} (95% CI: {hdi_low:.2f}-{hdi_high:.2f})")

# --- 3️⃣ Probability Heatmap ---
def plot_probability_heatmap():
    fe_grid = np.linspace(-0.5, 0.8, 50)
    teff_grid = np.linspace(-1, 1, 50)
    prob_grid = np.zeros((len(fe_grid), len(teff_grid)))

    for i, fe in enumerate(fe_grid):
        for j, teff in enumerate(teff_grid):
            log_lambda = alpha_samples + beta_met_samples*fe + beta_teff_samples*teff
            prob_grid[i,j] = (1 - np.exp(-np.exp(log_lambda))).mean()

    plt.figure(figsize=(8,5))
    plt.imshow(prob_grid.T, origin='lower', aspect='auto',
               extent=[fe_grid.min(), fe_grid.max(), teff_grid.min(), teff_grid.max()],
               cmap='viridis')
    plt.colorbar(label='Probability of ≥1 Planet')
    plt.xlabel('[Fe/H]')
    plt.ylabel('Normalized Teff')
    plt.title('Predicted Planet Probability Heatmap')
    plt.show()

# --- 4️⃣ Dynamic Priors Visualization ---
def explore_priors(alpha_sigma=0.5, beta_sigma=0.5):
    x = np.linspace(-3,3,100)
    plt.figure(figsize=(6,3))
    plt.plot(x, 1/(np.sqrt(2*np.pi*alpha_sigma**2)) * np.exp(-0.5*(x/alpha_sigma)**2), label=f'alpha prior σ={alpha_sigma}')
    plt.plot(x, 1/(np.sqrt(2*np.pi*beta_sigma**2)) * np.exp(-0.5*(x/beta_sigma)**2), label=f'beta prior σ={beta_sigma}')
    plt.title("Dynamic Priors Visualization")
    plt.legend()
    plt.show()

# ===============================
# Interactive controls
# ===============================

# 3D Posterior Explorer button
button_3d = widgets.Button(description="Show 3D Posterior Explorer")
output_3d = widgets.Output()
def on_button_clicked(b):
    with output_3d:
        output_3d.clear_output()
        plot_3d_explorer()
button_3d.on_click(on_button_clicked)
display(button_3d, output_3d)

# Scenario simulation sliders
fe_slider = widgets.FloatSlider(min=-0.5, max=0.8, value=0.2, step=0.05, description="[Fe/H]")
teff_slider = widgets.FloatSlider(min=3000, max=7500, value=6000, step=50, description="Teff K")
use_real_checkbox = widgets.Checkbox(value=True, description="Use Real Teff (K)")
widgets.interact(scenario_prediction, fe=fe_slider, teff=teff_slider, use_real=use_real_checkbox)

# Probability Heatmap button
button_heatmap = widgets.Button(description="Show Probability Heatmap")
output_heatmap = widgets.Output()
def on_heatmap_click(b):
    with output_heatmap:
        output_heatmap.clear_output()
        plot_probability_heatmap()
button_heatmap.on_click(on_heatmap_click)
display(button_heatmap, output_heatmap)

# Dynamic priors sliders
widgets.interact(
    explore_priors,
    alpha_sigma=widgets.FloatSlider(min=0.1,max=2,value=0.5,step=0.1,description="Alpha σ"),
    beta_sigma=widgets.FloatSlider(min=0.1,max=2,value=0.5,step=0.1,description="Beta σ")
)

import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.graph_objects as go
import arviz as az


from pyngrok import ngrok
ngrok.kill()  # stops all active tunnels

from pyngrok import ngrok
ngrok.kill()  # closes all active tunnels

# Commented out IPython magic to ensure Python compatibility.
# 1. Clone repo
!git clone https://github.com/spacey-g/EXOPLANET-_2.git
# %cd EXOPLANET-_2
!ls

# 2. Install dependencies
!pip install streamlit pyngrok

# 3. Set your ngrok authtoken (replace with your real token)

# 4. Import and configure ngrok
from pyngrok import ngrok
import os

script_path = "/content/EXOPLANET-_2/dashboard.py"
if not os.path.exists(script_path):
    raise FileNotFoundError(f"{script_path} not found. Check path.")

# 5. Open ngrok tunnel properly
public_url = ngrok.connect(8501, "http", bind_tls=True)
print(f"Streamlit app will be available at: {public_url}")

# 6. Run Streamlit app in background
get_ipython().system_raw(f"streamlit run {script_path} --server.port 8501 --server.headless true &")

from google.colab import drive
drive.mount('/content/drive')

merged_stars.to_csv('/content/drive/MyDrive/merged_stars.csv', index=False)



# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/spacey-g/EXOPLANET-_2.git
# %cd EXOPLANET-_2
!ls

# Commented out IPython magic to ensure Python compatibility.
# 1. Clone repo
!git clone https://github.com/spacey-g/EXOPLANET-_2.git
# %cd EXOPLANET-_2
!ls

# 2. Install dependencies
!pip install streamlit pyngrok

# 3. Set your ngrok authtoken (replace with your real token)
# 4. Import and configure ngrok
from pyngrok import ngrok
import os

script_path = "/content/EXOPLANET-_2/dashboard.py"
if not os.path.exists(script_path):
    raise FileNotFoundError(f"{script_path} not found. Check path.")

# 5. Open ngrok tunnel properly
public_url = ngrok.connect(8501, "http", bind_tls=True)
print(f"Streamlit app will be available at: {public_url}")

# 6. Run Streamlit app in background
get_ipython().system_raw(f"streamlit run {script_path} --server.port 8501 --server.headless true &")

import streamlit as st
import matplotlib.pyplot as plt
import pandas as pd
# Example: load CSV or DataFrame already in Colab
merged_stars = pd.read_csv('/content/clean_exoplanets.csv')

# Histogram: Stellar Temperature
fig, ax = plt.subplots()
ax.hist(merged_stars['st_teff'].dropna(), bins=30, edgecolor='black')
ax.set_xlabel("Effective Temperature (K)")
ax.set_ylabel("Number of Stars")
ax.set_title("Distribution of Stellar Temperatures")
st.pyplot(fig)

# Histogram: Stellar Metallicity
fig, ax = plt.subplots()
ax.hist(merged_stars['st_met'].dropna(), bins=30, edgecolor='black')
ax.set_xlabel("Metallicity [Fe/H]")
ax.set_ylabel("Number of Stars")
ax.set_title("Distribution of Stellar Metallicities")
st.pyplot(fig)

# Commented out IPython magic to ensure Python compatibility.
# ## works from here. use this to deploy app
# 
# %%writefile app.py
# import streamlit as st
# PAGE_CONFIG = {"page_title":"StColab.io","page_icon":":smiley:","layout":"centered"}
# st.beta_set_page_config(**PAGE_CONFIG)
# 
# 
# def main():
# 	st.title("Awesome Streamlit for ML")
# 	st.subheader("How to run streamlit from colab")
# 
# 
# 	menu = ["Home","About"]
# 	choice = st.sidebar.selectbox('Menu',menu)
# 	if choice == 'Home':
# 		st.subheader("Streamlit From Colab")
# 
# 
# 
# if __name__ == '__main__':
# 	main()

!ls

!nohub streamlit run app.py &

# Setup a tunnel to the streamlit port 8501
public_url = ngrok.connect(8501)
public_url

#tis is where delopying ends.use till this.

# the code for dashboard starts below. I was just checking the methods to deploy the dashboard in the previous cells with the first draft of copy.
# It didnt work so i just deleted those blocks. kept this here so i could prolly use it for deploying when it actualy works

# ================================================
# BLOCK 1: Create the Streamlit Dashboard (Colab-safe)
# ================================================

# 1️⃣ Install dependencies
!pip install -q streamlit plotly seaborn

# 2️⃣ Write dashboard code into a file
app_code = """
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
import seaborn as sns
import os

# ---------------------------
# Dashboard setup
# ---------------------------
st.set_page_config(
    page_title="Stars & Exoplanets Dashboard",
    page_icon="✨",
    layout="wide"
)

# ---------------------------
# Load data
# ---------------------------
@st.cache_data
def load_data():
    if os.path.exists('final_dataset_pre_bayesian.csv'):
        return pd.read_csv('final_dataset_pre_bayesian.csv')
    return None

df = load_data()

# ---------------------------
# Sidebar navigation
# ---------------------------
tabs = [
    "Home / Overview",
    "Kids Zone 👧🧒",
    "Light Curve Explorer 🔭",
    "Bayesian Models 📊",
    "Summary / Insights 🌌"
]
selected_tab = st.sidebar.radio("Navigate", tabs)

# =====================================================
# TAB 1️⃣: HOME / OVERVIEW — Data Analysis
# =====================================================
if selected_tab == "Home / Overview":
    st.title("🌟 Explore the World of Stars and Exoplanets!")
    st.markdown("Welcome! This dashboard lets you explore variable stars, their brightness, and TESS observations.")
    st.markdown("**Fun Fact:** Some stars blink like fireflies! ✨")

    if df is not None:
        # Dataset Overview
        st.markdown("---")
        st.markdown("## 📊 Dataset Overview")
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("Total Stars", len(df))
        col2.metric("Total Planets", int(df['planet_count'].sum()))
        col3.metric("Kepler Stars", len(df[df['mission']=='Kepler']))
        col4.metric("TESS Stars", len(df[df['mission']=='TESS']))

        # Survey Pie Chart
        st.markdown("### 🛰️ Survey Contribution")
        col1, col2 = st.columns(2)
        with col1:
            mission_counts = df['mission'].value_counts()
            fig = px.pie(values=mission_counts.values, names=mission_counts.index,
                         title="Kepler vs TESS", color_discrete_sequence=px.colors.sequential.Viridis)
            st.plotly_chart(fig, use_container_width=True)
        with col2:
            st.markdown("#### 📈 Key Statistics")
            st.metric("Avg Planets/Star", f"{df['planet_count'].mean():.2f}")
            st.metric("Max Planets/Star", int(df['planet_count'].max()))
            st.metric("Multi-Planet Systems", len(df[df['planet_count'] > 1]))

        # Histograms
        st.markdown("### 📊 Distribution Analysis")
        col1, col2, col3 = st.columns(3)

        with col1:
            fig = px.histogram(df, x='planet_count', title="Planet Count Distribution",
                               color_discrete_sequence=['#636EFA'])
            st.plotly_chart(fig, use_container_width=True)

        with col2:
            fig = px.histogram(df, x='st_met', nbins=30, title="Stellar Metallicity [Fe/H]",
                               color_discrete_sequence=['teal'])
            fig.add_vline(x=0, line_dash="dash", line_color="red", annotation_text="Solar")
            st.plotly_chart(fig, use_container_width=True)

        with col3:
            fig = px.histogram(df, x='st_teff', nbins=30, title="Stellar Temperature (K)",
                               color_discrete_sequence=['orange'])
            st.plotly_chart(fig, use_container_width=True)

        # Correlation Heatmap
        st.markdown("### 🔥 Correlation Heatmap")
        corr_cols = ['st_teff', 'st_met', 'st_mass', 'st_rad', 'planet_count']
        corr_matrix = df[corr_cols].dropna().corr()
        fig = go.Figure(data=go.Heatmap(
            z=corr_matrix.values,
            x=['Temperature', 'Metallicity', 'Mass', 'Radius', 'Planet Count'],
            y=['Temperature', 'Metallicity', 'Mass', 'Radius', 'Planet Count'],
            colorscale='RdBu', zmid=0,
            text=np.round(corr_matrix.values, 2),
            texttemplate='%{text}',
            textfont={"size": 12}
        ))
        fig.update_layout(height=500)
        st.plotly_chart(fig, use_container_width=True)

        # Box Plots
        st.markdown("### 📦 Planet Count vs Stellar Properties")
        col1, col2 = st.columns(2)
        with col1:
            fig = px.box(df, x='planet_count', y='st_met',
                         title="Metallicity by Planet Count",
                         color='planet_count',
                         color_discrete_sequence=px.colors.sequential.Viridis)
            fig.update_layout(showlegend=False)
            st.plotly_chart(fig, use_container_width=True)

        with col2:
            fig = px.box(df, x='planet_count', y='st_teff',
                         title="Temperature by Planet Count",
                         color='planet_count',
                         color_discrete_sequence=px.colors.sequential.Plasma)
            fig.update_layout(showlegend=False)
            st.plotly_chart(fig, use_container_width=True)

        # Spectral Type Heatmap
        st.markdown("### 🌡️ Mean Planet Count by Type")
        if 'spectral_type' in df.columns and 'metallicity_class' in df.columns:
            pivot = df.pivot_table(values='planet_count', index='spectral_type',
                                   columns='metallicity_class', aggfunc='mean')
            fig = go.Figure(data=go.Heatmap(
                z=pivot.values, x=pivot.columns, y=pivot.index,
                colorscale='Viridis',
                text=np.round(pivot.values, 2),
                texttemplate='%{text}',
                textfont={"size": 14}
            ))
            fig.update_layout(title="Average Planets per Star by Type", height=400)
            st.plotly_chart(fig, use_container_width=True)

        # Data Table
        st.markdown("### 🔍 Data Explorer")
        display_cols = ['hostname', 'st_teff', 'st_met', 'st_mass', 'st_rad',
                        'planet_count', 'mission', 'spectral_type']
        available_cols = [col for col in display_cols if col in df.columns]
        st.dataframe(df[available_cols].head(100), use_container_width=True, height=300)

        # Statistical Summary
        st.markdown("### 📈 Statistical Summary")
        st.dataframe(df[['st_teff', 'st_met', 'st_mass', 'st_rad', 'planet_count']].describe())
    else:
        st.error("⚠️ Data not found")
"""

# Save to file
with open("streamlit_app.py", "w") as f:
    f.write(app_code)

print("✅ Streamlit dashboard saved to streamlit_app.py")

!streamlit run streamlit_app.py --server.port 8501 --server.headless true &

!pip install pyngrok -q
from pyngrok import ngrok
public_url = ngrok.connect(8501)
public_url

# ================================================
# BLOCK 2: Add Bayesian Models tab
# ================================================

# We'll append new code to the existing app file
bayesian_code = """
# =====================================================
# TAB 4️⃣: BAYESIAN MODELS 📊
# =====================================================
elif selected_tab == "Bayesian Models 📊":
    st.title("📊 Bayesian Models for Exoplanet Occurrence")

    if df is None:
        st.error("⚠️ Data not found. Please upload or generate the dataset first.")
    else:
        st.markdown(
            "This section runs simplified Bayesian models using PyMC to understand how "
            "stellar properties like metallicity and temperature influence the number of planets per star."
        )

        # User can select model type
        model_choice = st.radio(
            "Select Model Type:",
            ["Single Predictor", "Multi Predictor", "Hierarchical (Simulated)"]
        )

        # We import only inside this tab to keep load time low
        import pymc as pm
        import arviz as az
        import matplotlib.pyplot as plt

        # To keep Streamlit fast, we'll use smaller sample sizes
        sample_kwargs = dict(draws=1000, tune=500, target_accept=0.9, progressbar=False)

        # Normalize predictors
        df_model = df.dropna(subset=['planet_count','st_met','st_teff']).copy()
        df_model['st_met_norm'] = (df_model['st_met'] - df_model['st_met'].mean()) / df_model['st_met'].std()
        df_model['st_teff_norm'] = (df_model['st_teff'] - df_model['st_teff'].mean()) / df_model['st_teff'].std()

        if model_choice == "Single Predictor":
            st.subheader("🔹 Single-Predictor Model")
            st.code("λ = exp(α + β_met × metallicity)")

            with st.spinner("Running PyMC model..."):
                with pm.Model() as single_model:
                    alpha = pm.Normal("alpha", mu=0, sigma=1)
                    beta_met = pm.Normal("beta_met", mu=0, sigma=1)
                    lambda_ = pm.math.exp(alpha + beta_met * df_model["st_met_norm"].values)
                    pm.Poisson("y_obs", mu=lambda_, observed=df_model["planet_count"])
                    idata_single = pm.sample(**sample_kwargs, return_inferencedata=True)

            st.success("✅ Model complete")
            st.dataframe(az.summary(idata_single, var_names=["alpha","beta_met"]))
            fig = az.plot_trace(idata_single, var_names=["alpha","beta_met"], compact=True)
            st.pyplot(fig[0].figure)

        elif model_choice == "Multi Predictor":
            st.subheader("🔹 Multi-Predictor Model")
            st.code("λ = exp(α + β_met × metallicity + β_teff × temperature)")

            with st.spinner("Running PyMC model..."):
                with pm.Model() as multi_model:
                    alpha = pm.Normal("alpha", mu=0, sigma=1)
                    beta_met = pm.Normal("beta_met", mu=0, sigma=1)
                    beta_teff = pm.Normal("beta_teff", mu=0, sigma=1)
                    lambda_ = pm.math.exp(
                        alpha
                        + beta_met * df_model["st_met_norm"].values
                        + beta_teff * df_model["st_teff_norm"].values
                    )
                    pm.Poisson("y_obs", mu=lambda_, observed=df_model["planet_count"])
                    idata_multi = pm.sample(**sample_kwargs, return_inferencedata=True)

            st.success("✅ Model complete")
            st.dataframe(az.summary(idata_multi, var_names=["alpha","beta_met","beta_teff"]))
            fig = az.plot_trace(idata_multi, var_names=["alpha","beta_met","beta_teff"], compact=True)
            st.pyplot(fig[0].figure)

        elif model_choice == "Hierarchical (Simulated)":
            st.subheader("🔹 Hierarchical-Style Model (Mission effects)")
            st.code("α_mission ∼ Normal(μ_α, σ_α)")

            if 'mission' not in df_model.columns:
                st.warning("Mission column not found — simulating Kepler/TESS split.")
                df_model['mission'] = np.random.choice(['Kepler','TESS'], len(df_model))

            mission_codes = pd.Categorical(df_model['mission']).codes
            n_missions = len(np.unique(mission_codes))

            with st.spinner("Running PyMC model..."):
                with pm.Model() as hier_model:
                    mu_alpha = pm.Normal("mu_alpha", 0, 1)
                    sigma_alpha = pm.Exponential("sigma_alpha", 1)
                    alpha = pm.Normal("alpha", mu_alpha, sigma_alpha, shape=n_missions)
                    beta_met = pm.Normal("beta_met", 0, 1)
                    lambda_ = pm.math.exp(alpha[mission_codes] + beta_met * df_model["st_met_norm"].values)
                    pm.Poisson("y_obs", mu=lambda_, observed=df_model["planet_count"])
                    idata_hier = pm.sample(**sample_kwargs, return_inferencedata=True)

            st.success("✅ Hierarchical model complete")
            st.dataframe(az.summary(idata_hier, var_names=["mu_alpha","sigma_alpha","beta_met"]))
            fig = az.plot_trace(idata_hier, var_names=["mu_alpha","sigma_alpha","beta_met"], compact=True)
            st.pyplot(fig[0].figure)
"""

# Append the Bayesian tab code to streamlit_app.py
with open("streamlit_app.py", "a") as f:
    f.write(bayesian_code)

print("✅ Block 2 complete: Bayesian Models tab added to dashboard")

!streamlit run streamlit_app.py --server.port 8501 --server.headless true &

# ================================================
# BLOCK 3: Add Light Curve Explorer tab
# ================================================
!pip install -q lightkurve astroquery astropy

lightcurve_code = """
# =====================================================
# TAB 3️⃣: LIGHT CURVE EXPLORER 🔭
# =====================================================
elif selected_tab == "Light Curve Explorer 🔭":
    st.title("🔭 Light Curve Explorer")

    st.markdown(
        "Use this tool to explore the brightness variations of stars observed by **TESS** or **Kepler**. "
        "Enter a target name or TIC ID below to fetch and visualize its light curve."
    )

    from lightkurve import search_lightcurve
    from astropy import units as u

    target = st.text_input("Enter Target (e.g., 'TIC 141914082' or 'Kepler-10'):")

    if target:
        try:
            st.info("⏳ Searching for available light curves...")
            search_result = search_lightcurve(target)

            if len(search_result) == 0:
                st.error("❌ No light curves found for this target. Try another ID or name.")
            else:
                st.success(f"✅ Found {len(search_result)} light curve file(s). Downloading latest...")

                lc = search_result.download()
                st.write("**Mission:**", lc.mission)
                st.write("**Quarter/Sector:**", lc.sector or lc.quarter or "N/A")

                # --- Plot raw light curve ---
                st.markdown("### 📈 Raw Light Curve (Flux vs Time)")
                fig_raw = lc.plot(label="Raw Flux", normalize=False, show=False)
                st.pyplot(fig_raw.figure)

                # --- Normalized / Detrended light curve ---
                st.markdown("### 🔧 Normalized & Detrended Light Curve")
                lc_clean = lc.remove_nans().normalize().flatten(window_length=301)
                fig_clean = lc_clean.plot(label="Normalized Flux", show=False)
                st.pyplot(fig_clean.figure)

                # --- Periodogram ---
                st.markdown("### ⏱️ Periodogram (Power vs Period)")
                pg = lc_clean.to_periodogram(method="lombscargle", minimum_period=0.05*u.day, maximum_period=20*u.day)
                fig_pg = pg.plot(show=False)
                st.pyplot(fig_pg.figure)
                best_period = pg.period_at_max_power
                st.metric("Best Period (days)", f"{best_period.value:.4f}")

                # --- Phase-folded light curve ---
                st.markdown("### 🌗 Phase-Folded Light Curve at Best Period")
                folded_lc = lc_clean.fold(period=best_period)
                fig_fold = folded_lc.plot(label="Folded Light Curve", show=False)
                st.pyplot(fig_fold.figure)

                st.success("✅ Light curve analysis complete.")
        except Exception as e:
            st.error(f"⚠️ Error: {e}")
    else:
        st.info("👉 Enter a target name or TIC ID above to begin.")
"""

# Append Light Curve Explorer to the Streamlit app
with open("streamlit_app.py", "a") as f:
    f.write(lightcurve_code)

print("✅ Block 3 complete: Light Curve Explorer tab added.")

!streamlit run streamlit_app.py --server.port 8501 --server.headless true &

# streamlit_app_kids_zone.py
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import random

st.set_page_config(page_title="Kids Zone", layout="wide")

st.title("🌟 Kids Zone: Explore the Stars!")

# ------------------------------
# 1️⃣ Interactive Light Curve
# ------------------------------
st.header("Watch a Star Shine!")
st.write("Use the slider to see how the brightness of a star changes over time.")

# Generate example light curve data
days = np.arange(0, 30, 0.5)
brightness = 1 + 0.5 * np.sin(2 * np.pi * days / 5)  # simple periodic star

df_lightcurve = pd.DataFrame({"Day": days, "Brightness": brightness})

day_slider = st.slider("Select Day:", min_value=0.0, max_value=30.0, step=0.5)
current_brightness = np.interp(day_slider, df_lightcurve["Day"], df_lightcurve["Brightness"])

st.write(f"Brightness of the star on Day {day_slider}: **{current_brightness:.2f}**")

fig = px.line(df_lightcurve, x="Day", y="Brightness", title="Star Brightness Over Time")
fig.add_scatter(x=[day_slider], y=[current_brightness], mode="markers", marker=dict(size=12, color="red"))
st.plotly_chart(fig, use_container_width=True)

# ------------------------------
# 2️⃣ Fun Astronomy Facts
# ------------------------------
st.header("🌌 Fun Astronomy Facts!")
facts = [
    "A day on Venus is longer than a year on Venus!",
    "Neutron stars can spin 600 times per second!",
    "There are more stars in the universe than grains of sand on Earth!",
    "Betelgeuse is a red supergiant star that could explode someday!",
    "The Sun accounts for 99.86% of the mass in our Solar System!"
]

if st.button("Show me a fun fact!"):
    st.success(random.choice(facts))

# ------------------------------
# 3️⃣ Simple Star Quiz
# ------------------------------
st.header("🪐 Star Quiz Time!")
question = "Which star is brighter in the night sky?"
options = ["Sirius", "Polaris", "Betelgeuse", "Rigel"]
answer = "Sirius"

user_answer = st.radio(question, options)

if st.button("Check Answer"):
    if user_answer == answer:
        st.balloons()
        st.success("Correct! 🎉 Sirius is the brightest star in the night sky.")
    else:
        st.error(f"Oops! The correct answer is {answer}.")

tab1, tab2, tab3 = st.tabs(["Home", "Light Curve", "Kids Zone"])
with tab1:
    st.write("Home content")
with tab2:
    st.write("Light Curve content")
with tab3:
    st.write("Kids Zone content")

# streamlit_dashboard.py
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import random

st.set_page_config(page_title="Astronomy Dashboard", layout="wide")
st.title("🌌 Astronomy Dashboard")

# ------------------------------
# Create Tabs
# ------------------------------
tab_home, tab_bayesian, tab_lightcurve, tab_kids = st.tabs([
    "Home", "Bayesian Analysis", "Light Curve Analysis", "Kids Zone"
])

# ==============================
# 1️⃣ Home
# ==============================
with tab_home:
    st.header("Welcome to the Astronomy Dashboard!")
    st.write("""
    Explore light curves of stars, perform Bayesian analysis, and even have fun in the Kids Zone!
    """)

    st.subheader("Example Star Data")
    # Example table
    example_data = pd.DataFrame({
        "Star": ["Betelgeuse", "Sirius", "Polaris", "Rigel"],
        "Magnitude": [0.42, -1.46, 1.97, 0.12],
        "Type": ["Red Supergiant", "Main Sequence", "Cepheid", "Blue Supergiant"]
    })
    st.dataframe(example_data)

# ==============================
# 2️⃣ Bayesian Analysis
# ==============================
with tab_bayesian:
    st.header("Bayesian Analysis")
    st.write("""
    Here you can integrate Bayesian models for stellar parameter estimation.
    For now, this is a placeholder for your analysis.
    """)

    # Example: Simple Bayesian placeholder
    st.subheader("Posterior Probability Example")
    x = np.linspace(0, 1, 100)
    posterior = np.exp(-5 * (x-0.6)**2)  # fake posterior
    fig = px.line(x=x, y=posterior, labels={"x":"Parameter", "y":"Posterior"}, title="Posterior Distribution")
    st.plotly_chart(fig, use_container_width=True)

# ==============================
# 3️⃣ Light Curve Analysis
# ==============================
with tab_lightcurve:
    st.header("Interactive Light Curve Analysis")
    st.write("Use the slider to see how the brightness of a star changes over time.")

    # Generate example light curve
    days = np.arange(0, 30, 0.5)
    brightness = 1 + 0.5 * np.sin(2 * np.pi * days / 5)
    df_lc = pd.DataFrame({"Day": days, "Brightness": brightness})

    day_slider = st.slider("Select Day:", min_value=0.0, max_value=30.0, step=0.5)
    current_brightness = np.interp(day_slider, df_lc["Day"], df_lc["Brightness"])
    st.write(f"Brightness on Day {day_slider}: **{current_brightness:.2f}**")

    fig_lc = px.line(df_lc, x="Day", y="Brightness", title="Star Brightness Over Time")
    fig_lc.add_scatter(x=[day_slider], y=[current_brightness], mode="markers",
                       marker=dict(size=12, color="red"))
    st.plotly_chart(fig_lc, use_container_width=True)

# ==============================
# 4️⃣ Kids Zone
# ==============================
with tab_kids:
    st.header("🌟 Kids Zone: Explore the Stars!")

    # Interactive Light Curve
    st.subheader("Watch a Star Shine!")
    day_slider_kids = st.slider("Select Day (Kids Zone):", min_value=0.0, max_value=30.0, step=0.5, key="kids_slider")
    current_brightness_kids = np.interp(day_slider_kids, df_lc["Day"], df_lc["Brightness"])
    st.write(f"Brightness of the star on Day {day_slider_kids}: **{current_brightness_kids:.2f}**")

    fig_kids = px.line(df_lc, x="Day", y="Brightness", title="Star Brightness Over Time (Kids Zone)")
    fig_kids.add_scatter(x=[day_slider_kids], y=[current_brightness_kids], mode="markers",
                         marker=dict(size=12, color="red"))
    st.plotly_chart(fig_kids, use_container_width=True)

    # Fun Facts
    st.subheader("🌌 Fun Astronomy Facts!")
    facts = [
        "A day on Venus is longer than a year on Venus!",
        "Neutron stars can spin 600 times per second!",
        "There are more stars in the universe than grains of sand on Earth!",
        "Betelgeuse is a red supergiant star that could explode someday!",
        "The Sun accounts for 99.86% of the mass in our Solar System!"
    ]
    if st.button("Show me a fun fact!", key="fact_btn"):
        st.success(random.choice(facts))

    # Quiz
    st.subheader("🪐 Star Quiz Time!")
    question = "Which star is brighter in the night sky?"
    options = ["Sirius", "Polaris", "Betelgeuse", "Rigel"]
    answer = "Sirius"
    user_answer = st.radio(question, options, key="quiz_radio")
    if st.button("Check Answer", key="quiz_btn"):
        if user_answer == answer:
            st.balloons()
            st.success("Correct! 🎉 Sirius is the brightest star in the night sky.")
        else:
            st.error(f"Oops! The correct answer is {answer}.")
